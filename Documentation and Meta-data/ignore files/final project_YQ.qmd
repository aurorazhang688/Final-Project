---
title: "Final Project"
format: html
---


## Setup
```{python}
# Setup
import dask.dataframe as dd
import pandas as pd
import numpy as np
import pyreadstat
from sklearn.preprocessing import StandardScaler
from scipy.interpolate import interp1d
from dash import Dash, dcc, html, Input, Output, State
import plotly.graph_objects as go
import plotly.express as px
from dash.dash_table import DataTable
from sklearn.impute import KNNImputer
```

## Load Dataset
```{python}
# Load data
gss_data, meta = pyreadstat.read_sav(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\GSS7218_R3.sav")
labels_data = pd.read_excel(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\suggestions_modified.xlsx")
```

```{python}
print(gss_data.head())
print(labels_data.head())
```

```{python}
gss_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\gss_data.csv", index=False)
```

## Clean Data
```{python}
# Define columns to recode
column_recode_3to2 = ["COURTS", "RELITEN", "HELPFUL", "FAIR", "TRUST", "AGED", "FINALTER", "DIVLAW"]
column_recode_4othertomissing = ["GETAHEAD"]
column_recode_5othertomissing = ["PREMARSX", "XMARSEX", "HOMOSEX"]

# Recode columns
gss_data[column_recode_4othertomissing] = gss_data[column_recode_4othertomissing].replace(4, np.nan)
gss_data[column_recode_5othertomissing] = gss_data[column_recode_5othertomissing].replace(5, np.nan)

# Recode values in column_recode_3to2
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(3, 9992)
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(2, 9993)
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(9992, 2)
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(9993, 3)
```

```{python}
# Calculate the mean of each column by year
data_by_year = gss_data.groupby("YEAR").mean(numeric_only=True).reset_index()
print(data_by_year.columns)
data_by_year.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\data_by_year.csv", index=False)
```

```{python}
# Calculate missing values per column in 'data_by_year'
nan_count_per_column = data_by_year.isna().sum()

# Mark the missing data of each column in labels_data
labels_data['missing_count'] = labels_data['variable'].map(nan_count_per_column)
```

```{python}
# Print the result
print(labels_data.head())
labels_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\labels_data.csv", index=False)
```

```{python}
# Get the current column names
current_colnames = gss_data.columns

# Convert column names to lowercase and then capitalize the first letter
new_colnames = [col.lower().capitalize() for col in current_colnames]

# Prepare for EGA
gss_data_plot = gss_data.copy()

# Assign the new column names to the data frame
gss_data_plot.columns = new_colnames

# Set the first column name to "year"
gss_data_plot.columns.values[0] = "year"
```

```{python}
# Print the new column names to verify
print(gss_data_plot.columns)
print(gss_data_plot.head())
gss_data_plot.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\gss_data_plot.csv", index=False)
```

```{python}
# Remove the column at index 2 (equivalent to removing ID column)
gss_data_cleaned = gss_data.drop(gss_data.columns[1], axis=1)  

# Group by the 'YEAR' column and calculate the mean for each year, ignoring NaN values
data_mean_by_year = gss_data_cleaned.groupby('YEAR').mean(numeric_only=True).reset_index()
```


```{python}
# Get the current column names
current_colnames = data_mean_by_year.columns

# Convert column names to lowercase and then capitalize the first letter
new_colnames = [col.lower().capitalize() for col in current_colnames]

# Assign the new column names to the DataFrame
data_mean_by_year.columns = new_colnames
```

```{python}
# Display the updated DataFrame with new column names
print(data_mean_by_year.head())

# Sve the dataframe as .csv
data_mean_by_year.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\data_mean_by_year.csv", index=False)
```


```{python}
# Load the US Immigration data
us_immigration_data = pd.read_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\USImmigration.csv")

# Load the external datasets
us_gdp_data = pd.read_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\FREDGDP.csv")

# Load mobility data
mobility_data = pd.read_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\GSS level 2e.csv")

mobility_data = mobility_data[["year", "Mobility", "Mobilitystate"]]

# Rename the first column to "year"
gss_data.rename(columns={gss_data.columns[0]: "year"}, inplace=True)

# Merge mobility data with the gss data on "year"
gss_data = pd.merge(mobility_data, data_mean_by_year, on="year", how="left")

# Format the date and filter for October in the GDP data
us_gdp_data['Date'] = pd.to_datetime(us_gdp_data['DATE'], format='%Y-%m-%d')
us_gdp_data['Year'] = us_gdp_data['Date'].dt.year
us_gdp_data['Month'] = us_gdp_data['Date'].dt.month
us_gdp_data = us_gdp_data[us_gdp_data['Month'] == 10]
```

```{python}
# Verifying by printing the first few rows of each dataset
print(gss_data.head())
print(us_gdp_data.head())
print(us_immigration_data.head())
```

```{python}
# Sve the dataframe as .csv
gss_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\gss_data1.csv", index=False)
us_gdp_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\us_gdp_data.csv", index=False)
us_immigration_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\us_immigration_data.csv", index=False)
```


```{python}
# Sort data by 'year' or the relevant column before interpolation
gss_data2 = gss_data.sort_values(by='year')

# Check data types of columns to be interpolated
column_types = gss_data2[columns_to_interpolate].dtypes
print("Data types of columns to interpolate:")
print(column_types)

# Convert columns to numeric (force errors to NaN)
gss_data2[columns_to_interpolate] = gss_data2[columns_to_interpolate].apply(pd.to_numeric, errors='coerce')
```


```{python}
# Interpolation using polynomial method
columns_to_interpolate = [
    "Fair", "Trust", "Happy", "Helpful", "Mobility", "Mobilitystate", "Aged", 
    "Attend", "Conarmy", "Conbus", "Conclerg", "Coneduc", "Confed", "Confinan", 
    "Conjudge", "Conlabor", "Conlegis", "Conmedic", "Conpress", "Consci", "Contv", 
    "Courts", "Divlaw", "Finalter", "Finrela", "Getahead", "Hapmar", "Health", 
    "Homosex", "Life", "Nataid", "Natarms", "Natcity", "Natcrime", "Natdrug", 
    "Nateduc", "Natenvir", "Natfare", "Natheal", "Natrace", "News", "Pornlaw", 
    "Premarsx", "Reliten", "Satjob", "Incom16", "Income", "Rincome", "Partyid", 
    "Polviews", "Natspac", "Fund", "Fund16", "Spfund", "Class", "Satfin", "Coop", 
    "Comprend", "Xmarsex"
]

# Apply interpolation using polynomial method
gss_data2[columns_to_interpolate] = gss_data2[columns_to_interpolate].interpolate(method='polynomial', order=2)

# Scaling the data (using StandardScaler from sklearn)
scaler = StandardScaler()

# Apply scaling to the selected columns
gss_data2[columns_to_interpolate] = scaler.fit_transform(gss_data2[columns_to_interpolate])

```

```{python}
# Merging datasets (left join)
gss_data3 = gss_data2.merge(us_immigration_data, how='left', left_on='year', right_on='Year') \
                   .rename(columns={'Number': 'Immigration'}) \
                   .merge(us_gdp_data, how='left', left_on='year', right_on='Year')

# Creating lag columns
columns_to_lag = [
    "Fair", "Trust", "Happy", "Helpful", "Mobility", "Mobilitystate", "Aged", "Attend", "Conarmy",
    "Conbus", "Conclerg", "Coneduc", "Confed", "Confinan", "Conjudge", "Conlabor", "Conlegis",
    "Conmedic", "Conpress", "Consci", "Contv", "Courts", "Divlaw", "Finalter", "Finrela", "Getahead",
    "Hapmar", "Health", "Homosex", "Life", "Nataid", "Natarms", "Natcity", "Natcrime", "Natdrug",
    "Nateduc", "Natenvir", "Natfare", "Natheal", "Natrace", "News", "Pornlaw", "Premarsx", "Reliten",
    "Satjob", "Incom16", "Income", "Rincome", "Partyid", "Polviews", "Natspac", "Fund", "Fund16",
    "Spfund", "Class", "Satfin", "Coop", "Comprend", "Xmarsex"
]

# Creating lag columns for each column in columns_to_lag
for column in columns_to_lag:
    gss_data3[f'{column}Lag'] = gss_data3[column].shift(1)

# Calculating statistics (can be adjusted as needed)
calculateStatistics_data = gss_data3
```

```{python}
print(calculateStatistics_data.head())
calculateStatistics_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\calculateStatistics_data.csv", index=False)
```