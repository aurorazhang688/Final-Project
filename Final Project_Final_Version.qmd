---
title: "Final Project"
format: html
---

## Group Members:
Name / Cnetid / Github_username / section

1.Pei-Chin Lu / Peichin / 

2. Yuan Qi / yuanqi / freyaqi

3. Huiting(Aurora) Zhang / zhanght / aurorazhang688 / section 4


## Research Question and the approach we took
Research in various fields has shown that residential mobility influences key aspects of how individuals think about themselves, interact with others, and perceive public rules. Based on this, our project primarily investigates the relationship between social mobility (e.g., economic opportunities, migration patterns, and educational access) and a range of personal characteristics. Specifically, we will pay attention to residents' happiness, trustiness, and perception of fairness. Establishing reliable connections between these factors is crucial for designing effective public policies that enhance social creativity and public well-being. To achieve this, we conduct a series of regression analyses. Additionally, to ensure that our variable selection is free from selection bias or "cherry-picking," we have implemented additional measures. We developed a method to objectively verify the validity of our variable selection, including the use of Exploratory Graph Analysis (EGA) and statistical tests, to classify variables as "Interested," "Proximate," or "Distal." These measures and visualizations help enhance the credibility of our research and ensure that our conclusions are derived from objective data analysis. All these charts and visualizations will ultimately be presented in our Shiny app.


## Setup
```{python}
# Setup
import dask.dataframe as dd
import pandas as pd
import numpy as np
import pyreadstat
from sklearn.preprocessing import StandardScaler
from scipy.interpolate import interp1d
from dash import Dash, dcc, html, Input, Output, State
import plotly.graph_objects as go
import plotly.express as px
from dash.dash_table import DataTable
from sklearn.impute import KNNImputer

from shiny import App, ui, reactive, render
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import statsmodels.api as sm
import tempfile
import matplotlib.pyplot as plt
```

## Load Dataset
```{python}
# Load data
gss_data, meta = pyreadstat.read_sav(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\GSS7218_R3.sav")
labels_data = pd.read_excel(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\suggestions_modified.xlsx")
```

We utilize publicly available datasets such as the General Social Survey (GSS) from 1978 to 2018, which provides comprehensive data on social trends, public happiness, and socio-economic factors in the United States. In addition, we will incorporate data on U.S. Immigration and GDP, sourced from government and financial databases like the Federal Reserve Economic Data (FRED). These datasets will be merged and preprocessed within our Shiny app to ensure consistency and compatibility. 

In labels_data, we categorized each personal characteristic variable into one of the following types for user selection in the Shiny app interface:
Likert Scale Variables,
Binary Variables,
Continuous Variables,
Multichoice Variables,
Administration Variables.

```{python}
print(gss_data.head())
print(labels_data.head())
```

## Clean Data
```{python}
# Define columns to recode
column_recode_3to2 = ["COURTS", "RELITEN", "HELPFUL", "FAIR", "TRUST", "AGED", "FINALTER", "DIVLAW"]
column_recode_4othertomissing = ["GETAHEAD"]
column_recode_5othertomissing = ["PREMARSX", "XMARSEX", "HOMOSEX"]

# Recode columns
gss_data[column_recode_4othertomissing] = gss_data[column_recode_4othertomissing].replace(4, np.nan)
gss_data[column_recode_5othertomissing] = gss_data[column_recode_5othertomissing].replace(5, np.nan)

# Recode values in column_recode_3to2
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(3, 9992)
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(2, 9993)
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(9992, 2)
gss_data[column_recode_3to2] = gss_data[column_recode_3to2].replace(9993, 3)
```

```{python}
# Calculate the mean of each column by year
data_by_year = gss_data.groupby("YEAR").mean(numeric_only=True).reset_index()
print(data_by_year.columns)
data_by_year.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\data_by_year.csv", index=False)
```

We cleaned the GSS dataset by recoding specific variables to ensure consistency and handle missing values. Columns with specific values (e.g., 4 or 5) were recoded to NaN, while others had their values swapped to align with our analysis needs. After preprocessing, we calculated the mean of each column grouped by year and saved the results to a CSV file for further analysis.

```{python}
# Calculate missing values per column in 'data_by_year'
nan_count_per_column = data_by_year.isna().sum()

# Mark the missing data of each column in labels_data
labels_data['missing_count'] = labels_data['variable'].map(nan_count_per_column)
```

```{python}
# Print the result
print(labels_data.head())
labels_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\labels_data.csv", index=False)
```

We calculated the number of missing values for each column in data_by_year and mapped these counts to the corresponding variables in labels_data.

```{python}
# Get the current column names
current_colnames = gss_data.columns

# Convert column names to lowercase and then capitalize the first letter
new_colnames = [col.lower().capitalize() for col in current_colnames]

# Prepare for EGA
gss_data_plot = gss_data.copy()

# Assign the new column names to the data frame
gss_data_plot.columns = new_colnames

# Set the first column name to "year"
gss_data_plot.columns.values[0] = "year"
```

```{python}
# Print the new column names to verify
print(gss_data_plot.columns)
print(gss_data_plot.head())
gss_data_plot.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\\gss_data_plot.csv", index=False)
```

```{python}
# Remove the column at index 2 (equivalent to removing ID column)
gss_data_cleaned = gss_data.drop(gss_data.columns[1], axis=1)  

# Group by the 'YEAR' column and calculate the mean for each year, ignoring NaN values
data_mean_by_year = gss_data_cleaned.groupby('YEAR').mean(numeric_only=True).reset_index()
```


```{python}
# Get the current column names
current_colnames = data_mean_by_year.columns

# Convert column names to lowercase and then capitalize the first letter
new_colnames = [col.lower().capitalize() for col in current_colnames]

# Assign the new column names to the DataFrame
data_mean_by_year.columns = new_colnames
```

```{python}
# Display the updated DataFrame with new column names
print(data_mean_by_year.head())

# Sve the dataframe as .csv
data_mean_by_year.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\data_mean_by_year.csv", index=False)
```


```{python}
# Load the US Immigration data
us_immigration_data = pd.read_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\USImmigration.csv")

# Load the external datasets
us_gdp_data = pd.read_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\FREDGDP.csv")

# Load mobility data
mobility_data = pd.read_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\GSS level 2e.csv")

mobility_data = mobility_data[["year", "Mobility", "Mobilitystate"]]
```


```{python}
# Change the name of data_mean_by_year
gss_data1 = data_mean_by_year.copy()

# Rename the first column to "year"
gss_data1.rename(columns={gss_data1.columns[0]: "year"}, inplace=True)

# Uniform the data type of year
gss_data1['year'] = gss_data1['year'].astype(int)
mobility_data['year'] = mobility_data['year'].astype(int)
```

```{python}
# Merge mobility data with the gss data on "year"
gss_data1 = pd.merge(mobility_data, gss_data1, on="year", how="left")
```


```{python}
# Format the date and filter for October in the GDP data
us_gdp_data['Date'] = pd.to_datetime(us_gdp_data['DATE'], format='%Y-%m-%d')
us_gdp_data['Year'] = us_gdp_data['Date'].dt.year
us_gdp_data['Month'] = us_gdp_data['Date'].dt.month
us_gdp_data = us_gdp_data[us_gdp_data['Month'] == 10]
```

```{python}
# Verifying by printing the first few rows of each dataset
print(gss_data.head())
print(us_gdp_data.head())
print(us_immigration_data.head())
```

```{python}
# Save the dataframe as .csv
gss_data1.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\gss_data1.csv", index=False)
us_gdp_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\us_gdp_data.csv", index=False)
us_immigration_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\us_immigration_data.csv", index=False)
```

We standardized column names, cleaned data by removing unnecessary columns, and calculated yearly averages. External datasets like US Immigration, GDP, and mobility data were processed and merged with GSS data using the "year" column. GDP data was filtered for October observations, ensuring all datasets were aligned and ready for analysis.


```{python}
# Define the columns to interpolate
columns_to_interpolate = [
    "Fair", "Trust", "Happy", "Helpful", "Mobility", "Mobilitystate", "Aged", 
    "Attend", "Conarmy", "Conbus", "Conclerg", "Coneduc", "Confed", "Confinan", 
    "Conjudge", "Conlabor", "Conlegis", "Conmedic", "Conpress", "Consci", "Contv", 
    "Courts", "Divlaw", "Finalter", "Finrela", "Getahead", "Hapmar", "Health", 
    "Homosex", "Life", "Nataid", "Natarms", "Natcity", "Natcrime", "Natdrug", 
    "Nateduc", "Natenvir", "Natfare", "Natheal", "Natrace", "News", "Pornlaw", 
    "Premarsx", "Reliten", "Satjob", "Incom16", "Income", "Rincome", "Partyid", 
    "Polviews", "Natspac", "Fund", "Fund16", "Spfund", "Class", "Satfin", "Coop", 
    "Comprend", "Xmarsex"
]
```

```{python}
# Sort data by 'year' or the relevant column before interpolation
gss_data2 = gss_data1.sort_values(by='year')

# Check data types of columns to be interpolated
column_types = gss_data2[columns_to_interpolate].dtypes
print("Data types of columns to interpolate:")
print(column_types)

# Convert columns to numeric (force errors to NaN)
gss_data2[columns_to_interpolate] = gss_data2[columns_to_interpolate].apply(pd.to_numeric, errors='coerce')
```

```{python}
# Apply interpolation using KNN imputer method
imputer = KNNImputer(n_neighbors=5) 
gss_data2[columns_to_interpolate] = imputer.fit_transform(gss_data2[columns_to_interpolate])
```


```{python}
# Scaling the data (using StandardScaler from sklearn)
scaler = StandardScaler()

# Apply scaling to the selected columns
gss_data2[columns_to_interpolate] = scaler.fit_transform(gss_data2[columns_to_interpolate])
```



```{python}
# Merging datasets (left join)
gss_data3 = gss_data2.merge(us_immigration_data, how='left', left_on='year', right_on='Year') \
                   .rename(columns={'Number': 'Immigration'}) \
                   .merge(us_gdp_data, how='left', left_on='year', right_on='Year')

# Creating lag columns
columns_to_lag = [
    "Fair", "Trust", "Happy", "Helpful", "Mobility", "Mobilitystate", "Aged", "Attend", "Conarmy",
    "Conbus", "Conclerg", "Coneduc", "Confed", "Confinan", "Conjudge", "Conlabor", "Conlegis",
    "Conmedic", "Conpress", "Consci", "Contv", "Courts", "Divlaw", "Finalter", "Finrela", "Getahead",
    "Hapmar", "Health", "Homosex", "Life", "Nataid", "Natarms", "Natcity", "Natcrime", "Natdrug",
    "Nateduc", "Natenvir", "Natfare", "Natheal", "Natrace", "News", "Pornlaw", "Premarsx", "Reliten",
    "Satjob", "Incom16", "Income", "Rincome", "Partyid", "Polviews", "Natspac", "Fund", "Fund16",
    "Spfund", "Class", "Satfin", "Coop", "Comprend", "Xmarsex"
]

# Creating lag columns for each column in columns_to_lag
for column in columns_to_lag:
    gss_data3[f'{column}Lag'] = gss_data3[column].shift(1)

# Calculating statistics (can be adjusted as needed)
calculateStatistics_data = gss_data3
```

```{python}
print(calculateStatistics_data.head())
calculateStatistics_data.to_csv(r"C:\Users\freya\Desktop\24 fall study\Python2\Final Project\calculateStatistics_data.csv", index=False)
```

We prepared the data by interpolating missing values using KNN imputation, standardizing the variables for consistency, and merging external datasets like immigration and GDP data to add context. Lagged variables were created to capture temporal relationships, ensuring the dataset was ready for regression and statistical analysis.


```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.impute import KNNImputer
from sklearn.cluster import KMeans
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Load data paths
cleaned_data_path = r"C:\Users\Pei-Chin\Dropbox (1)\final_python\calculateStatistics_data_test.csv"
labels_data_path = r"C:\Users\Pei-Chin\Dropbox (1)\final_python\labels_data.csv"

# Load cleaned data
def load_cleaned_data():
    data = pd.read_csv(cleaned_data_path)
    data.columns = data.columns.str.upper()  # Ensure all columns are uppercase
    data = data.apply(pd.to_numeric, errors="coerce")  # Ensure all columns are numeric
    return data

# Load labels data
def load_labels():
    labels = pd.read_csv(labels_data_path)
    return labels

# Data loading
cleaned_data = load_cleaned_data()
labels_data = load_labels()

# Function to filter variables
def filter_variables(variable_types, missing_threshold):
    type_column_map = {
        "Likert Scale Variables": "Likert Scale Variables",
        "Binary Variables": "Binary Variables",
        "Continuous Variables": "Continuous Variables",
        "Multichoice Variables": "Multichoice variables",
        "Administration Variable": "Administration variable",
    }
    selected_columns = [type_column_map[typ] for typ in variable_types if typ in type_column_map]
    if not selected_columns:
        return []

    filtered_data = labels_data.loc[
        (labels_data[selected_columns].sum(axis=1) > 0) &
        (labels_data["missing_count"] <= missing_threshold)
    ]
    return [var.upper() for var in filtered_data["variable"].tolist()]
```


```{python}
# Perform PCA and clustering
def perform_pca_and_clustering(selected_vars):
    if not selected_vars or len(selected_vars) < 2:
        print("Insufficient variables selected for PCA.")
        return None, None, None

    data_for_pca = cleaned_data[selected_vars]

    # Transpose data to cluster variables (columns)
    data_for_pca = data_for_pca.T

    # Fill missing values
    imputer = KNNImputer(n_neighbors=5)
    data_imputed = imputer.fit_transform(data_for_pca)

    # Standardize data
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_imputed)

    # Perform PCA
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(data_scaled)

    # Perform clustering
    kmeans = KMeans(n_clusters=4, random_state=42)
    clusters = kmeans.fit_predict(data_scaled)

    return pca_result, clusters, data_for_pca.index.tolist()

# Generate PCA plot
def generate_pca_plot(pca_result, clusters, variable_names):
    if pca_result is None or clusters is None:
        return

    plt.figure(figsize=(10, 7))
    for cluster in np.unique(clusters):
        cluster_indices = np.where(clusters == cluster)[0]
        plt.scatter(
            pca_result[cluster_indices, 0],
            pca_result[cluster_indices, 1],
            label=f"Cluster {cluster + 1}",
            alpha=0.7,
        )

        for idx in cluster_indices:
            plt.annotate(
                variable_names[idx],
                (pca_result[idx, 0], pca_result[idx, 1]),
                fontsize=8,
                alpha=0.7,
            )

    plt.title("PCA Result and Clustering")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.legend()
    plt.tight_layout()
    plt.show()
```
```{python}
# Perform regression
def perform_regression(selected_vars):
    if not selected_vars or len(selected_vars) < 2:
        print("Insufficient variables selected for regression.")
        return None

    results = []
    for var in selected_vars:
        lag_var = f"{var}Lag"
        if lag_var not in cleaned_data.columns:
            cleaned_data[lag_var] = cleaned_data[var].shift(1)

        if "MOBILITYLAG" not in cleaned_data.columns:
            print("Required variable 'MOBILITYLAG' not found in data.")
            continue

        regression_data = cleaned_data[[var, lag_var, "MOBILITYLAG"]].dropna()

        if regression_data.empty:
            continue

        X = regression_data[["MOBILITYLAG", lag_var]]
        y = regression_data[var]
        X = sm.add_constant(X)
        model = sm.OLS(y, X).fit()

        results.append({
            "Variable": var,
            "R-squared": model.rsquared,
            "MOBILITYLAG_coef": model.params["MOBILITYLAG"],
            "Lag_coef": model.params[lag_var],
            "p-value (MOBILITYLAG)": model.pvalues["MOBILITYLAG"],
            "t-value": model.tvalues["MOBILITYLAG"]
        })

    return pd.DataFrame(results)

# Generate cluster information
def generate_cluster_info(clusters, variable_names):
    cluster_info = {f"Cluster {i + 1}": [] for i in np.unique(clusters)}
    for var, cluster in zip(variable_names, clusters):
        cluster_info[f"Cluster {cluster + 1}"].append(var)
    return cluster_info

```

```{python}
# Plot regression fit lines
def plot_regression_fit_lines(cleaned_data, top_vars):
    plt.figure(figsize=(12, 8))
    for var, lag_var, model in top_vars:
        x_vals = np.linspace(cleaned_data["MOBILITYLAG"].min(), cleaned_data["MOBILITYLAG"].max(), 100)
        y_vals = model.params["const"] + model.params["MOBILITYLAG"] * x_vals
        plt.plot(x_vals, y_vals, label=f"{var} (Coef: {model.params['MOBILITYLAG']:.2f})", alpha=0.8)

    plt.title("Regression Fit Lines for Top 10 Variables", fontsize=14)
    plt.xlabel("MOBILITYLAG", fontsize=12)
    plt.ylabel("Fitted Value", fontsize=12)
    plt.legend(loc="best", fontsize=10)
    plt.tight_layout()
    plt.show()

```


```{python}
# Generate barplot and raincloud plot
def generate_plots(regression_results, clusters, variable_names, interested_vars):
    if regression_results is None:
        return

    # Assign categories
    results = []
    for var in variable_names:
        if var in interested_vars:
            category = "Interested"
        elif clusters[variable_names.index(var)] == clusters[variable_names.index(interested_vars[0])]:
            category = "Proximate"
        else:
            category = "Distal"

        t_value = regression_results.loc[regression_results["Variable"] == var, "MOBILITYLAG_coef"].abs().values[0]
        results.append({"variable": var, "abs_t_value": abs(t_value), "category": category})

    results_df = pd.DataFrame(results)

    # Define soft colors for categories
    color_palette = {
        "Interested": "#a6cee3",  # Soft blue
        "Proximate": "#b2df8a",  # Soft green
        "Distal": "#fb9a99",     # Soft pink
    }

    # Barplot
    plt.figure(figsize=(12, 8))
    sns.barplot(
        data=results_df,
        x="abs_t_value",
        y="variable",
        hue="category",
        order=results_df.sort_values("abs_t_value", ascending=False)["variable"],
        palette=color_palette  # Use the soft color palette
    )
    plt.title("Variables by Absolute T-Value and Category")
    plt.xlabel("Absolute T-Value")
    plt.ylabel("Variables")
    plt.legend(title="Category")
    plt.tight_layout()
    plt.show()


```


```{python}
# Generate barplot and raincloud plot
def generate_plots(regression_results, clusters, variable_names, interested_vars):
    if regression_results is None:
        return

    # Assign categories
    results = []
    for var in variable_names:
        if var in interested_vars:
            category = "Interested"
        elif clusters[variable_names.index(var)] == clusters[variable_names.index(interested_vars[0])]:
            category = "Proximate"
        else:
            category = "Distal"

        t_value = regression_results.loc[regression_results["Variable"] == var, "MOBILITYLAG_coef"].abs().values[0]
        results.append({"variable": var, "abs_t_value": abs(t_value), "category": category})

    results_df = pd.DataFrame(results)

    # Define soft colors for categories
    color_palette = {
        "Interested": "#a6cee3",  # Soft blue
        "Proximate": "#b2df8a",  # Soft green
        "Distal": "#fb9a99",     # Soft pink
    }


    # Raincloud Plot
    plt.figure(figsize=(10, 6))
    sns.violinplot(
        data=results_df,
        x="category",
        y="abs_t_value",
        hue="category",
        split=True,
        inner="box",
        palette=color_palette  # Use the soft color palette
    )
    sns.stripplot(
        data=results_df,
        x="category",
        y="abs_t_value",
        color="gray",  # Use neutral color for strip points
        size=2,
        alpha=0.5,
        jitter=True,
        dodge=True
    )
    plt.title("Raincloud Plot: Absolute T-Values by Category")
    plt.ylabel("Absolute T-Value for Regression Coefficient")
    plt.xlabel("Category")
    plt.legend(title="Category")
    plt.tight_layout()
    plt.show()
```


```{python}
# Main execution
if __name__ == "__main__":
    variable_types = ["Likert Scale Variables"]
    missing_threshold = 5

    # Filter variables
    selected_vars = filter_variables(variable_types, missing_threshold)
    print(f"Selected Variables: {selected_vars}")

    # Perform PCA and clustering
    pca_result, clusters, variable_names = perform_pca_and_clustering(selected_vars)

    # Display cluster information
    cluster_info = generate_cluster_info(clusters, variable_names)
    for cluster, vars_in_cluster in cluster_info.items():
        print(f"{cluster}: {vars_in_cluster}")

    # Generate PCA plot
    generate_pca_plot(pca_result, clusters, variable_names)

    # Perform regression
    regression_results = perform_regression(selected_vars)
    print(regression_results)

    # Select top 5 positive and 5 negative variables by MOBILITYLAG_coef
    regression_results["abs_coef"] = regression_results["MOBILITYLAG_coef"].abs()
    top_5_positive = regression_results.sort_values("MOBILITYLAG_coef", ascending=False).head(5)
    top_5_negative = regression_results.sort_values("MOBILITYLAG_coef", ascending=True).head(5)
    top_10_vars = pd.concat([top_5_positive, top_5_negative])

    # Prepare data for fit-line plot
    top_vars_for_fit = []
    for _, row in top_10_vars.iterrows():
        var = row["Variable"]
        lag_var = f"{var}Lag"
        regression_data = cleaned_data[[var, lag_var, "MOBILITYLAG"]].dropna()

        X = regression_data[["MOBILITYLAG", lag_var]]
        y = regression_data[var]
        X = sm.add_constant(X)
        model = sm.OLS(y, X).fit()

        top_vars_for_fit.append((var, lag_var, model))

    # Plot regression fit lines
    plot_regression_fit_lines(cleaned_data, top_vars_for_fit)

    # Generate barplot and raincloud plot
    generate_plots(regression_results, clusters, variable_names, ["HAPPY", "TRUST", "FAIR"])

```

